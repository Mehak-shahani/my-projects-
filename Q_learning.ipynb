{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx88MqLd4Ombs3K3axX9x/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehak-shahani/my-projects-/blob/main/Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qUlT7NLmeuON"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import gym  # For simulation environment\n",
        "import numpy as np  # For numerical operations\n",
        "import random  # For random operations in exploration-exploitation\n",
        "import torch  # For neural networks and tensors\n",
        "import torch.nn as nn  # For building deep neural networks\n",
        "import torch.optim as optim  # For optimization\n",
        "import matplotlib.pyplot as plt  # For plotting graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the CartPole environment from OpenAI Gym\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Visualize the environment with a single random action\n",
        "state = env.reset()\n",
        "env.render()  # Show the environment's visual\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_vw_3-ue-GW",
        "outputId": "011e5bc4-a1bb-4178-8c7f-5d3a45fd3039"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Q-learning agent class\n",
        "class QLearningAgent:\n",
        "    def __init__(self, action_space, state_space, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
        "        self.action_space = action_space\n",
        "        self.state_space = state_space\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.q_table = np.zeros((state_space, action_space))  # Initialize the Q-table with zeros\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # Exploration vs exploitation\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return env.action_space.sample()  # Exploration: Random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Exploitation: Action with the highest Q-value\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        # Q-learning update rule\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        self.q_table[state, action] = self.q_table[state, action] + self.learning_rate * (reward + self.discount_factor * self.q_table[next_state, best_next_action] - self.q_table[state, action])\n",
        "\n",
        "    def decay_exploration(self):\n",
        "        # Decay exploration rate over time\n",
        "        self.exploration_rate *= self.exploration_decay"
      ],
      "metadata": {
        "id": "6S4yCH5ehZYV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discretize the state space (CartPole's state space is continuous, so we need to discretize it)\n",
        "state_space = 100  # We will discretize the state into 100 bins\n",
        "action_space = env.action_space.n  # Number of possible actions in the environment\n",
        "\n",
        "# Create the Q-learning agent\n",
        "agent = QLearningAgent(action_space=action_space, state_space=state_space)"
      ],
      "metadata": {
        "id": "M5skenyPhba0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Q-learning agent\n",
        "episodes = 1000\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.digitize(state[0], bins=np.linspace(-2.4, 2.4, 10)) * 10 + np.digitize(state[1], bins=np.linspace(-3, 3, 10))  # Discretizing state space\n",
        "    total_reward = 0\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)  # Choose action based on policy\n",
        "        next_state, reward, done, info = env.step(action)  # Take action and observe the result (correct unpacking)\n",
        "        next_state = np.digitize(next_state[0], bins=np.linspace(-2.4, 2.4, 10)) * 10 + np.digitize(next_state[1], bins=np.linspace(-3, 3, 10))  # Discretizing next state\n",
        "\n",
        "        agent.update_q_table(state, action, reward, next_state)  # Update Q-table\n",
        "        state = next_state  # Move to the next state\n",
        "\n",
        "        total_reward += reward  # Accumulate reward\n",
        "\n",
        "    # Decay the exploration rate after each episode\n",
        "    agent.decay_exploration()\n",
        "     # Print progress every 100 episodes\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Exploration Rate = {agent.exploration_rate:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ3wHdYchgTR",
        "outputId": "a1e9186d-a954-4a97-ebc8-03b28de884e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100: Total Reward = 23.0, Exploration Rate = 0.0040\n",
            "Episode 200: Total Reward = 22.0, Exploration Rate = 0.0024\n",
            "Episode 300: Total Reward = 15.0, Exploration Rate = 0.0015\n",
            "Episode 400: Total Reward = 20.0, Exploration Rate = 0.0009\n",
            "Episode 500: Total Reward = 15.0, Exploration Rate = 0.0005\n",
            "Episode 600: Total Reward = 23.0, Exploration Rate = 0.0003\n",
            "Episode 700: Total Reward = 17.0, Exploration Rate = 0.0002\n",
            "Episode 800: Total Reward = 20.0, Exploration Rate = 0.0001\n",
            "Episode 900: Total Reward = 21.0, Exploration Rate = 0.0001\n",
            "Episode 1000: Total Reward = 15.0, Exploration Rate = 0.0000\n"
          ]
        }
      ]
    }
  ]
}